{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Open video file\n",
    "video_path = 'cropped_EyeTracking.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get video dimensions\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Get video frame rate\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps == 0:\n",
    "    fps = 30  # Set a default frame rate if retrieval fails\n",
    "\n",
    "frame_interval = int(1000 / fps)\n",
    "frames_to_show = int(0.5 * fps)  # Frames to display each point (0.5 seconds)\n",
    "\n",
    "# Simulate gaze points (for demonstration)\n",
    "num_points = 200\n",
    "gaze_points = [(random.randint(0, width), random.randint(0, height)) for _ in range(num_points)]\n",
    "\n",
    "# Initialize frame counter and heatmap list\n",
    "frame_count = 0\n",
    "heatmap_list = []  # To store the heatmaps for active points\n",
    "\n",
    "# Loop through video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Add new gaze point heatmap every frames_to_show frames\n",
    "    if frame_count < len(gaze_points):\n",
    "        point = gaze_points[frame_count]\n",
    "        heatmap = np.zeros((height, width), dtype=np.float32)\n",
    "        cv2.circle(heatmap, point, 30, 1.0, thickness=-1)  # Adjust radius for heat intensity\n",
    "        heatmap_list.append((heatmap, frames_to_show))  # Store heatmap with display duration\n",
    "\n",
    "    # Composite heatmaps and apply fading effect\n",
    "    combined_heatmap = np.zeros((height, width), dtype=np.float32)\n",
    "    for i, (heatmap, remaining_frames) in enumerate(heatmap_list):\n",
    "        if remaining_frames > 0:\n",
    "            heatmap_list[i] = (heatmap, remaining_frames - 1)  # Decrease remaining frames\n",
    "            combined_heatmap += heatmap * (remaining_frames / frames_to_show)  # Fade effect\n",
    "\n",
    "    # Apply Gaussian blur to the combined heatmap to smooth it out\n",
    "    heatmap_blurred = cv2.GaussianBlur(combined_heatmap, (0, 0), sigmaX=25, sigmaY=25, borderType=cv2.BORDER_DEFAULT)\n",
    "\n",
    "    # Normalize the heatmap to the range [0, 255]\n",
    "    heatmap_normalized = cv2.normalize(heatmap_blurred, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    # Apply a colormap to convert the grayscale heatmap to a color heatmap\n",
    "    heatmap_colored = cv2.applyColorMap(heatmap_normalized.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay the heatmap on the original frame\n",
    "    overlay = cv2.addWeighted(frame, 0.7, heatmap_colored, 0.3, 0)\n",
    "\n",
    "    # Display the video with the heatmap overlay\n",
    "    cv2.imshow('Video with Gaze Heatmap', overlay)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(frame_interval) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Increase frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Remove heatmaps that have fully faded out\n",
    "    heatmap_list = [(heatmap, remaining_frames) for heatmap, remaining_frames in heatmap_list if remaining_frames > 0]\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Images resize from 2560 * 720 to 1080 * 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "########################################################\n",
    "# GCD of image resolution (2560 x 720) is: 80\n",
    "# 9.0\n",
    "# 32.0\n",
    "########################################################\n",
    "\n",
    "# Define the directory where the images are stored\n",
    "image_dir = \"images_GCD2\"\n",
    "output_dir = \"images_GCD4_1080_360\"  # Where resized images will be saved\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define new dimensions\n",
    "new_width = 1080\n",
    "new_height = 360\n",
    "\n",
    "# Loop through the images\n",
    "for index in range(720, 746):  # Assuming index ranges from 1 to 100\n",
    "    # Construct the image file name\n",
    "    image_path = os.path.join(image_dir, f\"frame_{index}.png\")\n",
    "    output_path = os.path.join(output_dir, f\"frame_{index}.png\")\n",
    "\n",
    "    # Check if the image file exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Warning: {image_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load {image_path}.\")\n",
    "        continue\n",
    "\n",
    "    # Resize the image to half its original size\n",
    "    img_resized = cv2.resize(img, (img.shape[1] // 2, img.shape[0] // 2), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Resize the image to 1080x360 using cv2.resize()\n",
    "    img_resized = cv2.resize(img_resized, (new_width, new_height))\n",
    "\n",
    "    # Save the resized image to the output directory\n",
    "    cv2.imwrite(output_path, img_resized)\n",
    "\n",
    "    # print(f\"Resized {image_path} to {img_resized.shape[1]}x{img_resized.shape[0]} and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring for the 1080 * 360 images\n",
    "\n",
    "\n",
    "Step 1: Detect bounding box + save bounding_boxes_GCD4_1080_360. csv + save processed images with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/_0/_22pyjvn3s16kg6zx190bt1m0000gn/T/ipykernel_55688/171167187.py\", line 11, in <module>\n",
      "    from ultralytics import YOLO\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/__init__.py\", line 10, in <module>\n",
      "    from ultralytics.data.explorer.explorer import Explorer\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/data/__init__.py\", line 3, in <module>\n",
      "    from .base import BaseDataset\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/data/base.py\", line 15, in <module>\n",
      "    from torch.utils.data import Dataset\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jhpark/miniforge3/envs/baar/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Perform object detection on the image\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# results = model(image_path, show=False)  # 'show=False' if you don't want to display the image\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m; pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Save the processed image (getting the first image from results)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/engine/model.py:177\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/engine/model.py:453\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/engine/predictor.py:250\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 250\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/baar/lib/python3.9/site-packages/ultralytics/engine/predictor.py:127\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    125\u001b[0m     im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    130\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m im\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# uint8 to fp16/32\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Input: \n",
    "# - raw images 1080*360 pixels\n",
    "# - range\n",
    "# output: \n",
    "# - bounding_boxes_GCD4_1080_360.csv [index, x1, y1, x2, y2, confidence, class_id]\n",
    "# - processed_frame_{index}.png\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import cv2  # Ensure you have OpenCV imported\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define the directory where the images are stored\n",
    "image_dir = \"images_GCD4_1080_360\"\n",
    "output_dir = \"images_GCD4_1080_360\"\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Use the path to your YOLO model, e.g., \"yolov8n.pt\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_csv = \"bounding_boxes_GCD4_1080_360.csv\"\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Image\", \"x1\", \"y1\", \"x2\", \"y2\", \"Confidence\", \"Class\"])\n",
    "\n",
    "    # Loop through the images\n",
    "    for index in range(720, 746):  # Assuming index ranges from 720 to 745\n",
    "        # Construct the image file name\n",
    "        image_path = os.path.join(image_dir, f\"frame_{index}.png\")\n",
    "\n",
    "        # Check if the image file exists\n",
    "        if not os.path.isfile(image_path):\n",
    "            print(f\"Warning: {image_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Perform object detection on the image\n",
    "        # results = model(image_path, show=False)  # 'show=False' if you don't want to display the image\n",
    "        results = model(image_path, show=False, conf = 0.1) \n",
    "\n",
    "        # Save the processed image (getting the first image from results)\n",
    "        processed_image = results[0].orig_img  # Get the original image after processing\n",
    "\n",
    "        # Extract and draw bounding box coordinates on the image\n",
    "        for result in results:  # Iterate over the detection results\n",
    "            for box in result.boxes:  # Iterate over each box in the results\n",
    "                # Get bounding box coordinates, confidence, and class\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # Convert coordinates to integers\n",
    "                confidence = box.conf[0].item()  # Confidence score\n",
    "                class_id = int(box.cls[0].item())  # Class ID as an integer\n",
    "\n",
    "                # Draw the bounding box on the image\n",
    "                cv2.rectangle(processed_image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box, thickness 2\n",
    "                label = f\"Class: {class_id}, Conf: {confidence:.2f}\"\n",
    "                cv2.putText(processed_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([index, x1, y1, x2, y2, confidence, class_id])\n",
    "                # writer.writerow([f\"frame_{index}.png\", x1, y1, x2, y2, confidence, class_id])\n",
    "\n",
    "        # Save the processed image with bounding boxes\n",
    "        output_path = os.path.join(output_dir, f\"processed_frame_{index}.png\")\n",
    "        cv2.imwrite(output_path, processed_image)  # Save the image with the bounding boxes drawn\n",
    "\n",
    "print(f\"Bounding boxes saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Make the new merged bounding box (merging overlapping or close boxes), map the coordinates of the bounding boxes to the key, save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# - bounding_boxes_GCD4_1080_360.csv [index, x1, y1, x2, y2, confidence, class_id]\n",
    "# - range\n",
    "# output: \n",
    "# - merged_frame_{image_index}.png in \"/merged_bounding_boxes_images_GCD4_1080_360\"\n",
    "# - merged_bounding_boxes.pkl [frame_index*, merged_bounding_box_position (x1 y1 x2 y2)]\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import os  # Add this line to import the os module\n",
    "import pickle\n",
    "\n",
    "def get_bounding_box_center(x1, y1, x2, y2):\n",
    "    \"\"\"Calculate the center of a bounding box.\"\"\"\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    return center_x, center_y\n",
    "\n",
    "def boxes_overlap(box1, box2):\n",
    "    \"\"\"Check if two boxes overlap.\"\"\"\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    if x1_1 > x2_2 or x1_2 > x2_1:\n",
    "        return False\n",
    "    if y1_1 > y2_2 or y1_2 > y2_1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def merge_boxes(boxes, edge_threshold=50, range_threshold=50):\n",
    "    \"\"\"Merge all overlapping bounding boxes.\"\"\"\n",
    "    merged_boxes = []\n",
    "    used_boxes = [False] * len(boxes)\n",
    "    \n",
    "    for i, box1 in enumerate(boxes):\n",
    "        if used_boxes[i]:\n",
    "            continue\n",
    "        \n",
    "        merged_box = box1\n",
    "        used_boxes[i] = True\n",
    "        merged = True\n",
    "        \n",
    "        while merged:\n",
    "            merged = False\n",
    "            for j, box2 in enumerate(boxes):\n",
    "                if used_boxes[j]:\n",
    "                    continue\n",
    "                    \n",
    "                if boxes_overlap(merged_box, box2):\n",
    "                    merged_box = [\n",
    "                        min(merged_box[0], box2[0]),\n",
    "                        min(merged_box[1], box2[1]),\n",
    "                        max(merged_box[2], box2[2]),\n",
    "                        max(merged_box[3], box2[3])\n",
    "                    ]\n",
    "                    used_boxes[j] = True\n",
    "                    merged = True\n",
    "                \n",
    "                # Check for close but non-overlapping boxes\n",
    "                if not used_boxes[j]:\n",
    "                    x1_1, y1_1, x2_1, y2_1 = merged_box\n",
    "                    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "                    horizontal_edge_close = abs(x2_1 - x1_2) < edge_threshold or abs(x1_1 - x2_2) < edge_threshold\n",
    "                    vertical_edge_close = abs(y2_1 - y1_2) < edge_threshold or abs(y1_1 - y2_2) < edge_threshold\n",
    "\n",
    "                    if vertical_edge_close:\n",
    "                        left_right_in_range = abs(x1_1 - x1_2) < range_threshold and abs(x2_1 - x2_2) < range_threshold\n",
    "                        if left_right_in_range:\n",
    "                            merged_box = [\n",
    "                                min(merged_box[0], box2[0]),\n",
    "                                min(merged_box[1], box2[1]),\n",
    "                                max(merged_box[2], box2[2]),\n",
    "                                max(merged_box[3], box2[3])\n",
    "                            ]\n",
    "                            used_boxes[j] = True\n",
    "                            merged = True\n",
    "\n",
    "                    if horizontal_edge_close:\n",
    "                        top_bottom_in_range = abs(y1_1 - y1_2) < range_threshold and abs(y2_1 - y2_2) < range_threshold\n",
    "                        if top_bottom_in_range:\n",
    "                            merged_box = [\n",
    "                                min(merged_box[0], box2[0]),\n",
    "                                min(merged_box[1], box2[1]),\n",
    "                                max(merged_box[2], box2[2]),\n",
    "                                max(merged_box[3], box2[3])\n",
    "                            ]\n",
    "                            used_boxes[j] = True\n",
    "                            merged = True\n",
    "\n",
    "        merged_boxes.append(merged_box)\n",
    "    \n",
    "    return merged_boxes\n",
    "\n",
    "# (1) Load bounding boxes from CSV\n",
    "csv_file = \"bounding_boxes_GCD4_1080_360.csv\"\n",
    "bounding_boxes = {}\n",
    "\n",
    "with open(csv_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        image_index = row[\"Image\"]\n",
    "        x1, y1, x2, y2 = map(float, [row[\"x1\"], row[\"y1\"], row[\"x2\"], row[\"y2\"]])\n",
    "        if image_index not in bounding_boxes:\n",
    "            bounding_boxes[image_index] = []\n",
    "        bounding_boxes[image_index].append((x1, y1, x2, y2))\n",
    "\n",
    "# (2) Merge bounding boxes and store in a dictionary\n",
    "merged_bounding_boxes = {}\n",
    "\n",
    "for image_index, boxes in bounding_boxes.items():\n",
    "    # Perform box merging for each image\n",
    "    merged_boxes = merge_boxes(boxes)\n",
    "    merged_bounding_boxes[image_index] = merged_boxes\n",
    "\n",
    "# (3) Displaying and Saving the results\n",
    "output_dir = \"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/merged_bounding_boxes_images_GCD4_1080_360\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for image_index, merged_boxes in merged_bounding_boxes.items():\n",
    "    # Load the image\n",
    "    image_path = f\"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/images_GCD4_1080_360/processed_frame_{image_index}.png\"  # Modify this path accordingly\n",
    "    original_img = cv2.imread(image_path)\n",
    "\n",
    "    # Draw merged bounding boxes on the image\n",
    "    for (x1, y1, x2, y2) in merged_boxes:\n",
    "        cv2.rectangle(original_img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 255, 0), 2)  # Cyan color for merged boxes\n",
    "        # Ensure the coordinates are integers for putText\n",
    "        x1, y1 = int(x1), int(y1)\n",
    "        cv2.putText(original_img, \"merged\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "    # Save the result image\n",
    "    output_image_path = os.path.join(output_dir, f\"merged_frame_{image_index}.png\")\n",
    "    cv2.imwrite(output_image_path, original_img)\n",
    "\n",
    "    # Optionally display the image\n",
    "    # cv2.imshow(f\"Processed frame_{image_index}.png\", original_img)\n",
    "    # cv2.waitKey(0)\n",
    "\n",
    "# Optionally save merged bounding boxes and frame mappings to a pickle or JSON file\n",
    "import pickle\n",
    "with open('merged_bounding_boxes.pkl', 'wb') as f:\n",
    "    pickle.dump(merged_bounding_boxes, f)\n",
    "\n",
    "print(\"Merged bounding boxes saved and processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test for the HashMap merged_bounding_boxes.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "merged_boxes_file = \"merged_bounding_boxes.pkl\"\n",
    "\n",
    "with open(merged_boxes_file, 'rb') as f:\n",
    "    merged_bounding_boxes = pickle.load(f)\n",
    "\n",
    "for frame_index, boxes in merged_bounding_boxes.items():\n",
    "    print(f\"Frame Index: {frame_index}\")\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        print(f\"Bounding Box: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create Grid, Assign Scores, set detection bounding boxes, and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each individual frame, score grid + detection bounding box + save score.csv + save scored images (using coordinate as key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# - merged_bounding_boxes.pkl [frame_index*, merged_bounding_box_position x1 y1 x2 y2]\n",
    "# - range\n",
    "\n",
    "# output: \n",
    "# - grid_scores_1080_360.pkl:\n",
    "    # grid_scores_dict[coordinates] = {\n",
    "    #             'frame': index,\n",
    "    #             'grid_scores': grid_scores,\n",
    "    #             'bounding_boxes_with_scores': bounding_boxes_with_scores  # Store bounding boxes with score > 0\n",
    "    #         }\n",
    "# - \"processed_{image_name}\": scored images with merged bounding boxes and heatmap \n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Set paths\n",
    "image_dir = \"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/images_GCD4_1080_360\"\n",
    "output_dir = \"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/images_GCD4_1080_360_scored\"\n",
    "grid_scores_file = \"grid_scores_1080_360.pkl\"  # File to save grid scores\n",
    "frame_data_file = \"FrameandData.csv\"\n",
    "merged_boxes_file = \"merged_bounding_boxes.pkl\"\n",
    "\n",
    "# Grid parameters\n",
    "grid_size = 10\n",
    "img_width = 1080\n",
    "img_height = 360\n",
    "grid_width = img_width // grid_size\n",
    "grid_height = img_height // grid_size\n",
    "\n",
    "# Dictionary to store grid scores for each frame\n",
    "grid_scores_dict = {}\n",
    "\n",
    "# Load frame and coordinate mappings from CSV\n",
    "frame_to_coordinate = {}\n",
    "with open(frame_data_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        frame = int(row['Frame'])\n",
    "        x, y, angle = float(row['X_location(m)']), float(row['Y_location(m)']), float(row['Yaw_angle(deg)'])\n",
    "        frame_to_coordinate[frame] = (x, y, angle)\n",
    "\n",
    "# Load merged bounding boxes from the pickle file\n",
    "merged_bounding_boxes = {}\n",
    "with open(merged_boxes_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    for frame_index, boxes in data.items():\n",
    "        merged_bounding_boxes[frame_index] = boxes\n",
    "\n",
    "# Process each image based on merged bounding boxes\n",
    "for index in range(720, 746):  # Adjust the range according to your image set\n",
    "    image_name = f\"frame_{index}.png\"\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Image {image_name} does not exist. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load the image\n",
    "    original_img = cv2.imread(image_path)\n",
    "    if original_img is None:\n",
    "        print(f\"Failed to load {image_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Initialize grid scores\n",
    "    grid_scores = np.zeros((grid_height, grid_width))\n",
    "\n",
    "    # Get bounding boxes for the current image\n",
    "    boxes = merged_bounding_boxes.get(str(index), [])\n",
    "\n",
    "    # List to store bounding boxes with scores > 0\n",
    "    bounding_boxes_with_scores = []\n",
    "\n",
    "    # Calculate grid scores\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        x1_grid = int(x1 // grid_size)\n",
    "        y1_grid = int(y1 // grid_size)\n",
    "        x2_grid = int(x2 // grid_size)\n",
    "        y2_grid = int(y2 // grid_size)\n",
    "\n",
    "        box_has_score = False  # Flag to track if this bounding box has a score > 0\n",
    "\n",
    "        for i in range(y1_grid, y2_grid + 1):\n",
    "            for j in range(x1_grid, x2_grid + 1):\n",
    "                if i >= 0 and i < grid_height and j >= 0 and j < grid_width:\n",
    "                    if i == y1_grid or i == y2_grid or j == x1_grid or j == x2_grid:\n",
    "                        grid_scores[i, j] = max(grid_scores[i, j], 0.6)  # Partially filled grid\n",
    "                    else:\n",
    "                        grid_scores[i, j] = 1.0  # Fully filled grid\n",
    "\n",
    "                    # Check if this grid has a score > 0\n",
    "                    if grid_scores[i, j] > 0:\n",
    "                        box_has_score = True\n",
    "\n",
    "        # Adjacent grid penalty\n",
    "        for i in range(max(0, y1_grid - 1), min(grid_height, y2_grid + 2)):\n",
    "            for j in range(max(0, x1_grid - 1), min(grid_width, x2_grid + 2)):\n",
    "                if grid_scores[i, j] == 0:\n",
    "                    grid_scores[i, j] = 0.2  # Adjacent to a filled grid\n",
    "\n",
    "        # If this bounding box had any score > 0, add it to the list\n",
    "        if box_has_score:\n",
    "            bounding_boxes_with_scores.append((x1, y1, x2, y2))\n",
    "\n",
    "    # Normalize the grid scores\n",
    "    grid_scores = np.clip(grid_scores, 0, 1)\n",
    "\n",
    "    # Store the grid scores and bounding boxes with scores > 0 in the dictionary using the coordinates as the key\n",
    "    if index in frame_to_coordinate:\n",
    "        coordinates = frame_to_coordinate[index]\n",
    "        grid_scores_dict[coordinates] = {\n",
    "            'frame': index,\n",
    "            'grid_scores': grid_scores,\n",
    "            'bounding_boxes_with_scores': bounding_boxes_with_scores  # Store bounding boxes with score > 0\n",
    "        }\n",
    "\n",
    "    # Create a heatmap from grid scores\n",
    "    heatmap = cv2.resize(grid_scores, (img_width, img_height), interpolation=cv2.INTER_LINEAR)\n",
    "    heatmap_colored = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay the heatmap on the original image with transparency\n",
    "    overlay = cv2.addWeighted(original_img, 0.7, heatmap_colored, 0.3, 0)\n",
    "\n",
    "    # Draw bounding boxes on the overlay\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        cv2.rectangle(overlay, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Draw grid lines\n",
    "    for i in range(grid_size, img_width, grid_size):\n",
    "        cv2.line(overlay, (i, 0), (i, img_height), (255, 255, 255), 1)\n",
    "    for i in range(grid_size, img_height, grid_size):\n",
    "        cv2.line(overlay, (0, i), (img_width, i), (255, 255, 255), 1)\n",
    "\n",
    "    # Save the processed image\n",
    "    output_path = os.path.join(output_dir, f\"processed_{image_name}\")\n",
    "    cv2.imwrite(output_path, overlay)\n",
    "\n",
    "# Save the grid scores dictionary to a file using pickle\n",
    "with open(grid_scores_file, 'wb') as f:\n",
    "    pickle.dump(grid_scores_dict, f)\n",
    "\n",
    "print(\"Grid scores and bounding boxes saved to grid_scores_1080_360.pkl\")\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test for HashMap \"grid_scores_1080_360.pkl\" with coordinate, frame, and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved grid scores dictionary\n",
    "grid_scores_file = \"grid_scores_1080_360.pkl\"\n",
    "with open(grid_scores_file, 'rb') as f:\n",
    "    grid_scores_dict = pickle.load(f)\n",
    "\n",
    "# Print all contents of grid_scores_dict\n",
    "for coordinates, data in grid_scores_dict.items():\n",
    "    print(f\"Coordinates: {coordinates}, Frame: {data['frame']}\")\n",
    "    print(\"Grid scores:\")\n",
    "    print(data['grid_scores'])\n",
    "    print(\"bounding box\")\n",
    "    print(data['bounding_boxes_with_scores'])\n",
    "    print()  # Blank line for better readability\n",
    "\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # Load the pickle file\n",
    "# with open('grid_scores_1080_360.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "\n",
    "# def get_bounding_boxes_by_frame(data, target_frame):\n",
    "#     bounding_boxes = []\n",
    "\n",
    "#     # Iterate over each item in the dictionary\n",
    "#     for coordinate, details in data.items():\n",
    "#         if details['frame'] == target_frame:\n",
    "#             bounding_boxes = details['bounding_boxes_with_scores']\n",
    "#             break\n",
    "\n",
    "#     return bounding_boxes\n",
    "\n",
    "# # Example usage\n",
    "# target_frame_index = 735\n",
    "# bounding_boxes = get_bounding_boxes_by_frame(data, target_frame_index)\n",
    "\n",
    "# print(f\"Bounding boxes for frame {target_frame_index}:\")\n",
    "# for bbox in bounding_boxes:\n",
    "#     print(bbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: (Optional) Transform to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# - scored images with merged bounding boxes and heatmap\n",
    "# - rangeh\n",
    "# output: \n",
    "# - video\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Set the path to the directory where your images are located\n",
    "image_dir = \"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/images_GCD4_1080_360_scored\"\n",
    "output_video = \"output_video_GCD4_1080.mp4\"\n",
    "\n",
    "# Define the range of indices for the images you want to include in the video\n",
    "image_indices = range(720, 746)  # Change the range as per your requirement\n",
    "\n",
    "# Define video properties\n",
    "fps = 2  # Frames per second\n",
    "frame_size = (1080, 360)  # Width and height of the video (match the image resolution)\n",
    "\n",
    "# Initialize the video writer using the 'XVID' codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for widely supported AVI/MPEG-4 format\n",
    "video_writer = cv2.VideoWriter(output_video, fourcc, fps, frame_size)\n",
    "\n",
    "# Loop through the image indices\n",
    "for index in image_indices:\n",
    "    image_path = os.path.join(image_dir, f\"processed_frame_{index}.png\")\n",
    "\n",
    "    # Check if the image exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Image {image_path} does not exist. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Resize the image if it's not the same size as the frame size\n",
    "    if img.shape[1] != frame_size[0] or img.shape[0] != frame_size[1]:\n",
    "        img = cv2.resize(img, frame_size)\n",
    "\n",
    "    # Write the image to the video as a frame\n",
    "    video_writer.write(img)\n",
    "\n",
    "# Release the video writer\n",
    "video_writer.release()\n",
    "\n",
    "print(f\"Video saved as {output_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Loading bounding box and Cropping the corresponding parts (conservative way: all grids with score>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume based on frame to retrive bounding boxes\n",
    "\n",
    "# Input: \n",
    "# - grid_scores_1080_360.pkl:\n",
    "    # grid_scores_dict[coordinates] = {\n",
    "    #             'frame': index,\n",
    "    #             'grid_scores': grid_scores,\n",
    "    #             'bounding_boxes_with_scores': bounding_boxes_with_scores  # Store bounding boxes with score > 0\n",
    "    #         }\n",
    "# - frame_index\n",
    "\n",
    "# output: \n",
    "# - cropped images\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "# File paths\n",
    "image_dir = \"F:/OneDrive - The University of Texas at Austin/Research/Topic_BAAR/images_GCD4_1080_360\"\n",
    "grid_scores_file = \"grid_scores_1080_360.pkl\"\n",
    "output_dir = \"cropped_images\"  # Directory to save cropped images\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load grid scores dictionary and bounding boxes\n",
    "with open(grid_scores_file, 'rb') as f:\n",
    "    grid_scores_dict = pickle.load(f)\n",
    "\n",
    "# Function to find bounding boxes based on frame index\n",
    "def get_bounding_boxes_by_frame(data, target_frame):\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # Iterate over each item in the dictionary\n",
    "    for coordinate, details in data.items():\n",
    "        if details['frame'] == target_frame:\n",
    "            bounding_boxes = details['bounding_boxes_with_scores']\n",
    "            break\n",
    "\n",
    "    return bounding_boxes\n",
    "\n",
    "# Function to crop the image based on frame index and bounding box\n",
    "def crop_image(frame_index, bounding_box):\n",
    "    image_name = f\"frame_{frame_index}.png\"\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    \n",
    "    # Load the image\n",
    "    original_img = cv2.imread(image_path)\n",
    "    if original_img is None:\n",
    "        print(f\"Image {image_name} not found, skipping...\")\n",
    "        return None\n",
    "\n",
    "    # Get bounding box coordinates\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_img = original_img[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "    # Save the cropped image\n",
    "    cropped_image_name = f\"cropped_{image_name}\"\n",
    "    cropped_image_path = os.path.join(output_dir, cropped_image_name)\n",
    "    cv2.imwrite(cropped_image_path, cropped_img)\n",
    "    print(f\"Cropped image saved: {cropped_image_path}\")\n",
    "\n",
    "    return cropped_image_path\n",
    "\n",
    "# Specify the frame index\n",
    "frame_index = 735  # Replace with the actual frame index\n",
    "\n",
    "# Get the bounding boxes for the specified frame from the grid scores dictionary\n",
    "bounding_boxes = get_bounding_boxes_by_frame(grid_scores_dict, frame_index)\n",
    "\n",
    "if bounding_boxes is not None:\n",
    "    # Crop the image for each bounding box\n",
    "    for bounding_box in bounding_boxes:\n",
    "        crop_image(frame_index, bounding_box)\n",
    "else:\n",
    "    print(f\"No bounding boxes found for frame {frame_index}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
